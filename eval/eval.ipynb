{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A Cloud Guru](acg_logo.png)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjXARld2mkGH"
   },
   "source": [
    "<center><h1>Hyperparameter Tuning Job Created Using Amazon SageMaker</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab, you will learn how to import the iris dataset, split it into training and validation data, upload them to S3 bucket, fetch the lienar learner algorithm, initialize the estimator object, and automatically tune the hyperparameters using Amazon SageMaker's Automatic Model Tuning (AMT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use This Lab\n",
    "\n",
    "Most of the code is provided for you in this lab as our solution to the tasks presented. Some of the cells are left empty with a #TODO header and its your turn to fill in the empty code. You can always use our lab guide if you are stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Install dependencies and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "3MRfpjxBmeT8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.227.0)\n",
      "Requirement already satisfied: smdebug in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.0.34)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: shap in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (0.45.1)\n",
      "Collecting shap\n",
      "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.34.142)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (3.20.3)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.22.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.2.2)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (3.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyinstrument==3.4.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from smdebug) (3.4.2)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pyinstrument==3.4.2->smdebug) (0.2.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from shap) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from shap) (1.5.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from shap) (0.59.1)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.142 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.34.142)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->sagemaker) (2024.2.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.18.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from numba->shap) (0.42.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.16)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: shap\n",
      "  Attempting uninstall: shap\n",
      "    Found existing installation: shap 0.45.1\n",
      "    Uninstalling shap-0.45.1:\n",
      "      Successfully uninstalled shap-0.45.1\n",
      "Successfully installed shap-0.46.0\n"
     ]
    }
   ],
   "source": [
    "# Install Sagemaker\n",
    "!pip install -U sagemaker smdebug numpy==1.26.4 shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will use the iris dataset as our input data. \n",
    "2. The S3 bucket that you want to use for training data must be within the same region as the Notebook Instance.\n",
    "3. The IAM role arn is used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.\n",
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.debugger import Rule, DebuggerHookConfig, CollectionConfig, rule_configs\n",
    "\n",
    "# Initialize the SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Define the S3 bucket and prefix to store data\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "output_prefix = 'sagemaker/xgboost-debugger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fetch the IAM role using the get_execution_role function and assign the value to a variable `role`\n",
    "# Get the IAM role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Download the data and upload them to S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load_iris function is used to downlaod the input data\n",
    "2. The data is split into training and validatin data in the ration of 80 - 20\n",
    "3. The data is saved under 'train.csv' and 'validation.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "X, y = shap.datasets.adult()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the `train_test_split` function and the split the data in a 80 - 20 ratio. \n",
    "#TODO: Assign the values to variables `train_data` and `validation_data` \n",
    "# Split into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat(\n",
    "    [pd.Series(y_train, index=X_train.index, name=\"Income>50K\", dtype=int), X_train],\n",
    "    axis=1,\n",
    ")\n",
    "validation_data = pd.concat(\n",
    "    [pd.Series(y_test, index=X_test.index, name=\"Income>50K\", dtype=int), X_test],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "train_data.to_csv('train.csv', index=False, header=False)\n",
    "validation_data.to_csv('validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the upload_file function and upload the .csv files to the S3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('train.csv', output_bucket, f'{output_prefix}/train/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Using the strategy we followed to upload the training data as shown above, please upload the validation data to the output bucket.\n",
    "s3.upload_file('validation.csv', output_bucket, f'{output_prefix}/validation/validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Set up training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three separate variables that is dynamically constructed which will be used as one of the input parameters while generating training inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training files will be taken from: s3://sagemaker-us-east-1-016648419387/sagemaker/xgboost-debugger/train\n",
      "validation files will be taken from: s3://sagemaker-us-east-1-016648419387/sagemaker/xgboost-debugger/validation\n",
      "training artifacts output location: s3://sagemaker-us-east-1-016648419387/sagemaker/xgboost-debugger/output\n"
     ]
    }
   ],
   "source": [
    "# creating the inputs for the fit() function with the training and validation location\n",
    "s3_train_data = f\"s3://{output_bucket}/{output_prefix}/train\"\n",
    "print(f\"training files will be taken from: {s3_train_data}\")\n",
    "s3_validation_data = f\"s3://{output_bucket}/{output_prefix}/validation\"\n",
    "print(f\"validation files will be taken from: {s3_validation_data}\")\n",
    "output_location = f\"s3://{output_bucket}/{output_prefix}/output\"\n",
    "print(f\"training artifacts output location: {output_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the sagemaker.session.s3_input objects from our data channels. Note that we are using the content_type as text/csv. We use two channels here one for training and the second one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the session.s3_input() format for fit() accepted by the sdk\n",
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    record_wrapping=None,\n",
    "    compression=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Following the same strategy as shown above, please set up a training input for validation data.\n",
    "#TODO: Name it as `validation_data`\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_validation_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    record_wrapping=None,\n",
    "    compression=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Fetch the algorithm and initialize estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the image for the Linear Learner Algorithm according to the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.2-1\n"
     ]
    }
   ],
   "source": [
    "# getting the linear learner image according to the region\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "container = retrieve('xgboost', boto3.Session().region_name, version='1.2-1')\n",
    "print(container)\n",
    "deploy_amt_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an estimator from the SageMaker Python SDK using the Linear Learner container image and we setup the training parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"51\",\n",
    "}\n",
    "\n",
    "save_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job xgboost-iris-debugger-20240806-04-27-29\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig, CollectionConfig\n",
    "from sagemaker.estimator import Estimator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "job_name = \"xgboost-iris-debugger-\" + strftime(\"%Y%m%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "xgboost_estimator = Estimator(\n",
    "    role=role,\n",
    "    base_job_name=job_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    image_uri=container,\n",
    "    hyperparameters=hyperparameters,\n",
    "    max_run=1800,\n",
    "    debugger_hook_config=DebuggerHookConfig(\n",
    "        s3_output_path=output_location,  # Required\n",
    "        collection_configs=[\n",
    "            CollectionConfig(name=\"metrics\", parameters={\"save_interval\": str(save_interval)}),\n",
    "            CollectionConfig(\n",
    "                name=\"feature_importance\",\n",
    "                parameters={\"save_interval\": str(save_interval)},\n",
    "            ),            \n",
    "        ],\n",
    "    ),\n",
    "    rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                \"collection_names\": \"metrics\",\n",
    "                \"num_steps\": str(save_interval * 2),\n",
    "            },\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: xgboost-iris-debugger-20240806-04-27-29-2024-08-06-04-27-34-988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-06 04:27:36 Starting - Starting the training job...\n",
      "2024-08-06 04:27:59 Starting - Preparing the instances for trainingLossNotDecreasing: InProgress\n",
      "...\n",
      "2024-08-06 04:28:39 Downloading - Downloading input data...\n",
      "2024-08-06 04:28:59 Downloading - Downloading the training image......\n",
      "2024-08-06 04:30:02 Training - Training image download completed. Training in progress.\n",
      "2024-08-06 04:30:02 Uploading - Uploading generated training model\u001b[34m[2024-08-06 04:29:56.416 ip-10-0-185-38.ec2.internal:6 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[2024-08-06 04:29:56.543 ip-10-0-185-38.ec2.internal:6 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-08-06 04:29:56.544 ip-10-0-185-38.ec2.internal:6 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-08-06 04:29:56.544 ip-10-0-185-38.ec2.internal:6 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-08-06 04:29:56.545 ip-10-0-185-38.ec2.internal:6 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-08-06 04:29:56.545 ip-10-0-185-38.ec2.internal:6 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 26048 rows and 12 columns\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 6513 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.15314#011validation-error:0.15538\u001b[0m\n",
      "\u001b[34m[2024-08-06 04:29:56.576 ip-10-0-185-38.ec2.internal:6 INFO hook.py:413] Monitoring the collections: metrics, feature_importance, losses\u001b[0m\n",
      "\u001b[34m[2024-08-06 04:29:56.579 ip-10-0-185-38.ec2.internal:6 INFO hook.py:476] Hook is writing from the hook with pid: 6\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.14980#011validation-error:0.15308\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.14481#011validation-error:0.14617\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.14538#011validation-error:0.14663\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.14489#011validation-error:0.14755\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.14454#011validation-error:0.14878\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.14458#011validation-error:0.14786\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.14446#011validation-error:0.14709\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.14377#011validation-error:0.14678\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.14304#011validation-error:0.14617\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.14262#011validation-error:0.14479\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.14212#011validation-error:0.14356\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.14013#011validation-error:0.14233\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.14009#011validation-error:0.14248\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.14016#011validation-error:0.14218\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.13870#011validation-error:0.14172\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.13859#011validation-error:0.14141\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.13809#011validation-error:0.14095\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.13625#011validation-error:0.14003\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.13510#011validation-error:0.13895\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.13494#011validation-error:0.13865\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.13441#011validation-error:0.13650\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.13372#011validation-error:0.13588\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.13368#011validation-error:0.13588\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.13279#011validation-error:0.13404\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.13233#011validation-error:0.13266\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.13203#011validation-error:0.13235\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.13030#011validation-error:0.13158\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.12903#011validation-error:0.13020\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.12934#011validation-error:0.13005\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.12884#011validation-error:0.12928\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.12823#011validation-error:0.12851\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.12796#011validation-error:0.12759\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.12719#011validation-error:0.12744\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.12734#011validation-error:0.12759\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.12726#011validation-error:0.12728\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.12726#011validation-error:0.12774\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.12646#011validation-error:0.12744\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.12611#011validation-error:0.12728\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.12607#011validation-error:0.12636\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.12565#011validation-error:0.12728\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.12538#011validation-error:0.12667\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.12535#011validation-error:0.12744\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.12538#011validation-error:0.12698\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.12519#011validation-error:0.12728\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.12523#011validation-error:0.12713\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.12488#011validation-error:0.12698\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.12500#011validation-error:0.12698\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.12469#011validation-error:0.12667\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.12412#011validation-error:0.12713\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.12369#011validation-error:0.12621\u001b[0m\n",
      "\n",
      "2024-08-06 04:30:39 Completed - Training job completed\n",
      "Training seconds: 124\n",
      "Billable seconds: 124\n"
     ]
    }
   ],
   "source": [
    "xgboost_estimator.fit({\"train\": train_data, \"validation\": validation_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-06 04:36:38.161 ip-172-16-23-156.ec2.internal:8280 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2024-08-06 04:36:38.186 ip-172-16-23-156.ec2.internal:8280 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-016648419387/sagemaker/xgboost-debugger/output/xgboost-iris-debugger-20240806-04-27-29-2024-08-06-04-27-34-988/debug-output\n"
     ]
    }
   ],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "s3_output_path = xgboost_estimator.latest_job_debugger_artifacts_path()\n",
    "trial = create_trial(s3_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-06 04:36:53.421 ip-172-16-23-156.ec2.internal:8280 INFO trial.py:197] Training has ended, will refresh one final time in 1 sec.\n",
      "[2024-08-06 04:36:54.437 ip-172-16-23-156.ec2.internal:8280 INFO trial.py:210] Loaded all steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['feature_importance/cover/f0',\n",
       " 'feature_importance/cover/f1',\n",
       " 'feature_importance/cover/f10',\n",
       " 'feature_importance/cover/f11',\n",
       " 'feature_importance/cover/f2',\n",
       " 'feature_importance/cover/f3',\n",
       " 'feature_importance/cover/f4',\n",
       " 'feature_importance/cover/f5',\n",
       " 'feature_importance/cover/f6',\n",
       " 'feature_importance/cover/f7',\n",
       " 'feature_importance/cover/f8',\n",
       " 'feature_importance/cover/f9',\n",
       " 'feature_importance/gain/f0',\n",
       " 'feature_importance/gain/f1',\n",
       " 'feature_importance/gain/f10',\n",
       " 'feature_importance/gain/f11',\n",
       " 'feature_importance/gain/f2',\n",
       " 'feature_importance/gain/f3',\n",
       " 'feature_importance/gain/f4',\n",
       " 'feature_importance/gain/f5',\n",
       " 'feature_importance/gain/f6',\n",
       " 'feature_importance/gain/f7',\n",
       " 'feature_importance/gain/f8',\n",
       " 'feature_importance/gain/f9',\n",
       " 'feature_importance/total_cover/f0',\n",
       " 'feature_importance/total_cover/f1',\n",
       " 'feature_importance/total_cover/f10',\n",
       " 'feature_importance/total_cover/f11',\n",
       " 'feature_importance/total_cover/f2',\n",
       " 'feature_importance/total_cover/f3',\n",
       " 'feature_importance/total_cover/f4',\n",
       " 'feature_importance/total_cover/f5',\n",
       " 'feature_importance/total_cover/f6',\n",
       " 'feature_importance/total_cover/f7',\n",
       " 'feature_importance/total_cover/f8',\n",
       " 'feature_importance/total_cover/f9',\n",
       " 'feature_importance/total_gain/f0',\n",
       " 'feature_importance/total_gain/f1',\n",
       " 'feature_importance/total_gain/f10',\n",
       " 'feature_importance/total_gain/f11',\n",
       " 'feature_importance/total_gain/f2',\n",
       " 'feature_importance/total_gain/f3',\n",
       " 'feature_importance/total_gain/f4',\n",
       " 'feature_importance/total_gain/f5',\n",
       " 'feature_importance/total_gain/f6',\n",
       " 'feature_importance/total_gain/f7',\n",
       " 'feature_importance/total_gain/f8',\n",
       " 'feature_importance/total_gain/f9',\n",
       " 'feature_importance/weight/f0',\n",
       " 'feature_importance/weight/f1',\n",
       " 'feature_importance/weight/f10',\n",
       " 'feature_importance/weight/f11',\n",
       " 'feature_importance/weight/f2',\n",
       " 'feature_importance/weight/f3',\n",
       " 'feature_importance/weight/f4',\n",
       " 'feature_importance/weight/f5',\n",
       " 'feature_importance/weight/f6',\n",
       " 'feature_importance/weight/f7',\n",
       " 'feature_importance/weight/f8',\n",
       " 'feature_importance/weight/f9',\n",
       " 'train-error',\n",
       " 'validation-error']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TensorUnavailable",
     "evalue": "Tensor metrics was not saved.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTensorUnavailable\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/smdebug/trials/trial.py:226\u001b[0m, in \u001b[0;36mTrial.tensor\u001b[0;34m(self, tname)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors[tname]\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TensorUnavailable(tname)\n",
      "\u001b[0;31mTensorUnavailable\u001b[0m: Tensor metrics was not saved."
     ]
    }
   ],
   "source": [
    "trial.tensor(\"metrics\").values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "job_name = \"xgboost-iris-debugger-\" + strftime(\"%Y%m%d-%H-%M-%S\", gmtime())\n",
    "print(\"Training job\", job_name)\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Define hyperparameter ranges and invoke tuning job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the initial values for the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the set_hyperparameters function and set the initial hyperparameters on linear learner\n",
    "# feature_dim=4, predictor_type='regressor', mini_batch_size=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the Continous parameter range and define the `learning rate` and `wd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Define the hyperparameter ranges\n",
    "#1. 'learning_rate': ContinuousParameter(0.01, 0.2)\n",
    "#2. 'wd': ContinuousParameter(0.0, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Instead of manually configuring our hyper parameter values and training with SageMaker Training, we'll use Amazon SageMaker Automatic Model Tuning.\n",
    "2. The code sample below shows you how to use the HyperParameterTuner. It accepts the hyperparameter ranges we set previously.\n",
    "3. Based on your capacity, you can adjust the `max_jobs` and `max_parallel_jobs`\n",
    "4. The goal of the tuning job is to minimize `rmse`\n",
    "5. The tuning job will take 8 to 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a HyperparameterTuner object\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=linear,\n",
    "    objective_metric_name='validation:rmse',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'validation:rmse', 'Regex': 'validation rmse=([0-9\\\\.]+)'}\n",
    "    ],\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=2,\n",
    "    objective_type='Minimize'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initiate the tuner job by invoking the fit function.\n",
    "#2. Pass the train_data and validation_data as input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
